{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 99 - Modelo Solución Final\n",
        "\n",
        "Este notebook contiene la solución completa para la competencia de Kaggle.\n",
        "\n",
        "**Modelo:** Ensemble de Random Forest + LightGBM con weighted average\n",
        "\n",
        "**Pipeline:**\n",
        "1. Carga de datos\n",
        "2. Preprocesamiento (mapeo ordinal, encoding)\n",
        "3. Entrenamiento de Random Forest\n",
        "4. Entrenamiento de LightGBM\n",
        "5. Combinación mediante promedio ponderado\n",
        "6. Generación de predicciones para test\n",
        "\n",
        "**Accuracy en Kaggle:** 0.41497"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación e importación de librerías"
      ],
      "metadata": {
        "id": "install_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar LightGBM\n",
        "!pip install lightgbm -q"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de datos"
      ],
      "metadata": {
        "id": "load_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datasets\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Train: {df_train.shape}\")\n",
        "print(f\"Test: {df_test.shape}\")"
      ],
      "metadata": {
        "id": "load"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento\n",
        "\n",
        "El preprocesamiento incluye:\n",
        "- Mapeo ordinal para variables con orden natural (estrato, educación, horas de trabajo)\n",
        "- Encoding binario para variables Si/No\n",
        "- Label encoding para programa académico\n",
        "- One-hot encoding para departamento (Random Forest)\n",
        "- Label encoding para departamento (LightGBM)\n",
        "- Imputación de valores faltantes con mediana"
      ],
      "metadata": {
        "id": "preprocess_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_base(df, is_train=True):\n",
        "    \"\"\"\n",
        "    Preprocesamiento base común para ambos modelos.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    \n",
        "    # Guardar ID y target\n",
        "    ids = df_copy['ID'].copy()\n",
        "    if is_train:\n",
        "        target = df_copy['RENDIMIENTO_GLOBAL'].copy()\n",
        "    \n",
        "    # Eliminar columnas que no son features\n",
        "    cols_to_drop = ['ID']\n",
        "    if is_train:\n",
        "        cols_to_drop.append('RENDIMIENTO_GLOBAL')\n",
        "    df_copy = df_copy.drop(cols_to_drop, axis=1)\n",
        "    \n",
        "    # Mapeo ordinal para valor de matrícula\n",
        "    valor_orden = {\n",
        "        'Menos de 500 mil': 0,\n",
        "        'Entre 500 mil y menos de 1 millón': 1,\n",
        "        'Entre 1 millón y menos de 2.5 millones': 2,\n",
        "        'Entre 2.5 millones y menos de 4 millones': 3,\n",
        "        'Entre 4 millones y menos de 5.5 millones': 4,\n",
        "        'Entre 5.5 millones y menos de 7 millones': 5,\n",
        "        'Más de 7 millones': 6\n",
        "    }\n",
        "    df_copy['E_VALORMATRICULAUNIVERSIDAD'] = df_copy['E_VALORMATRICULAUNIVERSIDAD'].map(valor_orden)\n",
        "    \n",
        "    # Mapeo ordinal para horas de trabajo semanales\n",
        "    horas_orden = {\n",
        "        '0': 0,\n",
        "        'Menos de 10 horas': 1,\n",
        "        'Entre 11 y 20 horas': 2,\n",
        "        'Entre 21 y 30 horas': 3,\n",
        "        'Más de 30 horas': 4\n",
        "    }\n",
        "    df_copy['E_HORASSEMANATRABAJA'] = df_copy['E_HORASSEMANATRABAJA'].map(horas_orden)\n",
        "    \n",
        "    # Mapeo ordinal para estrato\n",
        "    estrato_map = {\n",
        "        'Sin Estrato': 0,\n",
        "        'Estrato 1': 1,\n",
        "        'Estrato 2': 2,\n",
        "        'Estrato 3': 3,\n",
        "        'Estrato 4': 4,\n",
        "        'Estrato 5': 5,\n",
        "        'Estrato 6': 6\n",
        "    }\n",
        "    df_copy['F_ESTRATOVIVIENDA'] = df_copy['F_ESTRATOVIVIENDA'].map(estrato_map)\n",
        "    \n",
        "    # Mapeo ordinal para educación de padres\n",
        "    educacion_orden = {\n",
        "        'Ninguno': 0,\n",
        "        'Primaria incompleta': 1,\n",
        "        'Primaria completa': 2,\n",
        "        'Secundaria (Bachillerato) incompleta': 3,\n",
        "        'Secundaria (Bachillerato) completa': 4,\n",
        "        'Técnica o tecnológica incompleta': 5,\n",
        "        'Técnica o tecnológica completa': 6,\n",
        "        'Educación profesional incompleta': 7,\n",
        "        'Educación profesional completa': 8,\n",
        "        'Postgrado': 9,\n",
        "        'No sabe': 2\n",
        "    }\n",
        "    df_copy['F_EDUCACIONPADRE'] = df_copy['F_EDUCACIONPADRE'].map(educacion_orden)\n",
        "    df_copy['F_EDUCACIONMADRE'] = df_copy['F_EDUCACIONMADRE'].map(educacion_orden)\n",
        "    \n",
        "    # Mapeo binario para variables Si/No\n",
        "    binary_map = {'Si': 1, 'No': 0, 'S': 1, 'N': 0}\n",
        "    binary_cols = ['F_TIENEINTERNET', 'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL', \n",
        "                   'F_TIENECOMPUTADOR', 'F_TIENEINTERNET.1', 'E_PAGOMATRICULAPROPIO']\n",
        "    \n",
        "    for col in binary_cols:\n",
        "        if col in df_copy.columns:\n",
        "            df_copy[col] = df_copy[col].map(binary_map)\n",
        "    \n",
        "    df_copy['E_PRIVADO_LIBERTAD'] = df_copy['E_PRIVADO_LIBERTAD'].map({'S': 1, 'N': 0})\n",
        "    \n",
        "    # Imputar valores faltantes con mediana\n",
        "    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if df_copy[col].isna().sum() > 0:\n",
        "            df_copy[col] = df_copy[col].fillna(df_copy[col].median())\n",
        "    \n",
        "    return df_copy, ids, target if is_train else None"
      ],
      "metadata": {
        "id": "preprocess_base"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_rf(df_base):\n",
        "    \"\"\"\n",
        "    Preprocesamiento específico para Random Forest.\n",
        "    Usa one-hot encoding para departamento.\n",
        "    \"\"\"\n",
        "    df_copy = df_base.copy()\n",
        "    \n",
        "    # Label encoding para programa académico\n",
        "    le_programa = LabelEncoder()\n",
        "    df_copy['E_PRGM_ACADEMICO'] = le_programa.fit_transform(df_copy['E_PRGM_ACADEMICO'].astype(str))\n",
        "    \n",
        "    # One-hot encoding para departamento\n",
        "    depto_dummies = pd.get_dummies(df_copy['E_PRGM_DEPARTAMENTO'], prefix='E_PRGM_DEPARTAMENTO', drop_first=False)\n",
        "    df_copy = df_copy.join(depto_dummies)\n",
        "    df_copy = df_copy.drop('E_PRGM_DEPARTAMENTO', axis=1)\n",
        "    \n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "preprocess_rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_lgbm(df_base):\n",
        "    \"\"\"\n",
        "    Preprocesamiento específico para LightGBM.\n",
        "    Usa label encoding para todas las categorías.\n",
        "    \"\"\"\n",
        "    df_copy = df_base.copy()\n",
        "    \n",
        "    # Label encoding para programa y departamento\n",
        "    le_programa = LabelEncoder()\n",
        "    le_depto = LabelEncoder()\n",
        "    \n",
        "    df_copy['E_PRGM_ACADEMICO'] = le_programa.fit_transform(df_copy['E_PRGM_ACADEMICO'].astype(str))\n",
        "    df_copy['E_PRGM_DEPARTAMENTO'] = le_depto.fit_transform(df_copy['E_PRGM_DEPARTAMENTO'].astype(str))\n",
        "    \n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "preprocess_lgbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar preprocesamiento base\n",
        "print(\"Preprocesando train...\")\n",
        "df_train_base, train_ids, target = preprocess_base(df_train, is_train=True)\n",
        "\n",
        "print(\"Preprocesando test...\")\n",
        "df_test_base, test_ids, _ = preprocess_base(df_test, is_train=False)\n",
        "\n",
        "# Crear versiones específicas para cada modelo\n",
        "print(\"\\nCreando versión para Random Forest...\")\n",
        "df_train_rf = preprocess_for_rf(df_train_base)\n",
        "df_test_rf = preprocess_for_rf(df_test_base)\n",
        "\n",
        "print(\"Creando versión para LightGBM...\")\n",
        "df_train_lgbm = preprocess_for_lgbm(df_train_base)\n",
        "df_test_lgbm = preprocess_for_lgbm(df_test_base)\n",
        "\n",
        "print(f\"\\nTrain RF: {df_train_rf.shape}\")\n",
        "print(f\"Train LightGBM: {df_train_lgbm.shape}\")"
      ],
      "metadata": {
        "id": "apply_preprocess"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación de datos para entrenamiento"
      ],
      "metadata": {
        "id": "prepare_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificar variable objetivo\n",
        "le_target = LabelEncoder()\n",
        "y_encoded = le_target.fit_transform(target)\n",
        "\n",
        "# Separar features y target para Random Forest\n",
        "X_rf = df_train_rf.values\n",
        "X_test_rf = df_test_rf.values\n",
        "\n",
        "# Separar features y target para LightGBM\n",
        "X_lgbm = df_train_lgbm.values\n",
        "X_test_lgbm = df_test_lgbm.values\n",
        "\n",
        "# Split train/validation con el mismo random_state para ambos modelos\n",
        "X_train_rf, X_val_rf, y_train, y_val = train_test_split(\n",
        "    X_rf, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "X_train_lgbm, X_val_lgbm, _, _ = train_test_split(\n",
        "    X_lgbm, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train_rf.shape[0]} muestras\")\n",
        "print(f\"Validation: {X_val_rf.shape[0]} muestras\")"
      ],
      "metadata": {
        "id": "prepare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento de Random Forest"
      ],
      "metadata": {
        "id": "rf_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Entrenando Random Forest...\")\n",
        "\n",
        "# Configuración optimizada para Colab\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    max_features='log2',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train_rf, y_train)\n",
        "\n",
        "# Evaluar en validación\n",
        "y_val_pred_rf = rf_model.predict(X_val_rf)\n",
        "rf_acc = accuracy_score(y_val, y_val_pred_rf)\n",
        "\n",
        "print(f\"\\nAccuracy Random Forest: {rf_acc:.5f}\")"
      ],
      "metadata": {
        "id": "train_rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento de LightGBM"
      ],
      "metadata": {
        "id": "lgbm_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Entrenando LightGBM...\\n\")\n",
        "\n",
        "# Crear datasets de LightGBM\n",
        "train_data = lgb.Dataset(X_train_lgbm, label=y_train)\n",
        "val_data = lgb.Dataset(X_val_lgbm, label=y_val, reference=train_data)\n",
        "\n",
        "# Configuración optimizada\n",
        "params_lgbm = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': 4,\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 64,\n",
        "    'learning_rate': 0.03,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'min_child_samples': 40,\n",
        "    'verbose': -1,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "lgbm_model = lgb.train(\n",
        "    params_lgbm,\n",
        "    train_data,\n",
        "    num_boost_round=300,\n",
        "    valid_sets=[val_data],\n",
        "    valid_names=['valid'],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=30), lgb.log_evaluation(period=50)]\n",
        ")\n",
        "\n",
        "# Evaluar en validación\n",
        "y_val_pred_lgbm_proba = lgbm_model.predict(X_val_lgbm)\n",
        "y_val_pred_lgbm = np.argmax(y_val_pred_lgbm_proba, axis=1)\n",
        "lgbm_acc = accuracy_score(y_val, y_val_pred_lgbm)\n",
        "\n",
        "print(f\"\\nAccuracy LightGBM: {lgbm_acc:.5f}\")"
      ],
      "metadata": {
        "id": "train_lgbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble - Weighted Average\n",
        "\n",
        "Combinamos las predicciones de ambos modelos usando un promedio ponderado.\n",
        "Los pesos se calculan basándose en el accuracy de cada modelo en validación."
      ],
      "metadata": {
        "id": "ensemble_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular pesos basados en accuracy\n",
        "total_acc = rf_acc + lgbm_acc\n",
        "weight_rf = rf_acc / total_acc\n",
        "weight_lgbm = lgbm_acc / total_acc\n",
        "\n",
        "print(f\"Pesos del ensemble:\")\n",
        "print(f\"  Random Forest: {weight_rf:.3f}\")\n",
        "print(f\"  LightGBM: {weight_lgbm:.3f}\")\n",
        "\n",
        "# Obtener probabilidades\n",
        "y_val_proba_rf = rf_model.predict_proba(X_val_rf)\n",
        "y_val_proba_lgbm = y_val_pred_lgbm_proba\n",
        "\n",
        "# Promedio ponderado de probabilidades\n",
        "y_val_proba_ensemble = (weight_rf * y_val_proba_rf) + (weight_lgbm * y_val_proba_lgbm)\n",
        "\n",
        "# Predicción final\n",
        "y_val_pred_ensemble = np.argmax(y_val_proba_ensemble, axis=1)\n",
        "\n",
        "# Evaluar ensemble\n",
        "ensemble_acc = accuracy_score(y_val, y_val_pred_ensemble)\n",
        "\n",
        "print(f\"\\nResultados en validación:\")\n",
        "print(f\"  Random Forest:  {rf_acc:.5f}\")\n",
        "print(f\"  LightGBM:       {lgbm_acc:.5f}\")\n",
        "print(f\"  Ensemble:       {ensemble_acc:.5f}\")"
      ],
      "metadata": {
        "id": "ensemble"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reporte de clasificación"
      ],
      "metadata": {
        "id": "report_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir a labels originales\n",
        "y_val_labels = le_target.inverse_transform(y_val)\n",
        "y_val_pred_labels = le_target.inverse_transform(y_val_pred_ensemble)\n",
        "\n",
        "print(\"Reporte de clasificación:\\n\")\n",
        "print(classification_report(y_val_labels, y_val_pred_labels))\n",
        "\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_val_labels, y_val_pred_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['alto', 'bajo', 'medio-alto', 'medio-bajo'],\n",
        "            yticklabels=['alto', 'bajo', 'medio-alto', 'medio-bajo'])\n",
        "plt.title('Matriz de Confusión - Ensemble')\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predicho')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "report"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento de modelos finales\n",
        "\n",
        "Entrenamos con todos los datos de train para hacer las predicciones finales."
      ],
      "metadata": {
        "id": "final_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Entrenando modelos finales con todos los datos...\\n\")\n",
        "\n",
        "# Random Forest con todos los datos\n",
        "print(\"[1/2] Random Forest...\")\n",
        "rf_final = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    max_features='log2',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "rf_final.fit(X_rf, y_encoded)\n",
        "\n",
        "# LightGBM con todos los datos\n",
        "print(\"[2/2] LightGBM...\")\n",
        "full_train_data = lgb.Dataset(X_lgbm, label=y_encoded)\n",
        "\n",
        "lgbm_final = lgb.train(\n",
        "    params_lgbm,\n",
        "    full_train_data,\n",
        "    num_boost_round=lgbm_model.best_iteration,\n",
        "    callbacks=[lgb.log_evaluation(period=0)]\n",
        ")\n",
        "\n",
        "print(\"\\nModelos finales entrenados\")"
      ],
      "metadata": {
        "id": "train_final"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de predicciones para test"
      ],
      "metadata": {
        "id": "predict_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generando predicciones para test...\\n\")\n",
        "\n",
        "# Predicciones de Random Forest\n",
        "test_proba_rf = rf_final.predict_proba(X_test_rf)\n",
        "\n",
        "# Predicciones de LightGBM\n",
        "test_proba_lgbm = lgbm_final.predict(X_test_lgbm)\n",
        "\n",
        "# Ensemble con weighted average\n",
        "test_proba_ensemble = (weight_rf * test_proba_rf) + (weight_lgbm * test_proba_lgbm)\n",
        "test_pred_encoded = np.argmax(test_proba_ensemble, axis=1)\n",
        "\n",
        "# Convertir a labels originales\n",
        "test_pred = le_target.inverse_transform(test_pred_encoded)\n",
        "\n",
        "# Crear archivo de submission\n",
        "submission = pd.DataFrame({\n",
        "    'ID': test_ids,\n",
        "    'RENDIMIENTO_GLOBAL': test_pred\n",
        "})\n",
        "\n",
        "# Guardar\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Archivo submission.csv generado\")\n",
        "print(f\"Total de predicciones: {len(submission)}\")\n",
        "print(f\"\\nDistribución de predicciones:\")\n",
        "print(submission['RENDIMIENTO_GLOBAL'].value_counts())\n",
        "print(f\"\\nPrimeras 10 filas:\")\n",
        "print(submission.head(10))"
      ],
      "metadata": {
        "id": "generate_submission"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resumen\n",
        "\n",
        "**Modelo utilizado:** Ensemble de Random Forest y LightGBM\n",
        "\n",
        "**Estrategia de combinación:** Weighted average basado en accuracy de validación\n",
        "\n",
        "**Resultados:**\n",
        "- Accuracy en validación mostrado arriba\n",
        "- Accuracy en Kaggle: 0.41497\n",
        "\n",
        "**Por qué funciona:**\n",
        "- Random Forest y LightGBM capturan patrones diferentes en los datos\n",
        "- Random Forest usa bootstrap sampling y es robusto\n",
        "- LightGBM usa gradient boosting y es más eficiente\n",
        "- La combinación ponderada aprovecha las fortalezas de ambos\n",
        "\n",
        "**Archivo generado:** submission.csv listo para enviar a Kaggle"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
